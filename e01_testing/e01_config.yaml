# =================================================================================================
# GENERAL SETTINGS
# =================================================================================================

# Possible values: 1/4 and 1/8
core_symmetry: 1/4

one_hot_encoding: True

train_split: 0.2

# Possible outputs:
# - keff_start
# - keff_max
# - ppf_start
# - ppf_max
# - ppf_end
# - cycle_length_in_days
# - rho_start
# - rho_max
# - keff_history
# - rho_history
output_columns:
  - cycle_length_in_days
  - keff_max


# =================================================================================================
# INPUT FILES SETTINGS
# =================================================================================================

input_output_file_details:
  file_path: data/input_output_data.csv
  
  create_new_cols:
    single_cols:
      rho_start: '1 - 1/df.keff_start'
      rho_max: '1 - 1/df.keff_max'

    multiple_cols:
      rhoN: '1 - 1/df.keffN'


monocore_file_details:
  file_path: data/monocores_dictionary.xlsx
  sheet_name: summary

  create_new_cols:
    single_cols:
      rho_start: '1 - 1/df.keff_start'
      rho_max: '1 - 1/df.keff_max'


# =================================================================================================
# MONOCORE SETTINGS
# =================================================================================================

use_monocores: True
core_number_column_name: monocore

# Possible transform colum names:
# - cycle_length_in_days
# - keff_max
# - Pxy_max
# - Pz_max
# - keff_start
# - PPF_start
# - PPF_max
# - PPF_end
# - rho_start
# - rho_max
transform_column_names:
  - cycle_length_in_days
  - Pxy_max
  - Pz_max
  - PPF_max
  - PPF_end
  - rho_start
  - rho_max


# =================================================================================================
# NEURAL NETWORK SETTINGS
# =================================================================================================

# Possible activation functions:
# - relu
# - sigmoid
# - softmax
# - softplus
# - softsign
# - tanh
# - selu
# - elu
# - exponential
# - linear

layers:
  layer1:
    neurons: 30
    activation: linear

  layer2:
    neurons: 30
    activation: linear

  layer_output:
    activation: linear

normalize: False
loss_function: mean_absolute_percentage_error
learning_rate: 0.000001
epochs: 1000


# =============================================================================
# EVALUATION OF THE MODEL
# =============================================================================

# Possible metrics:
# - mean_absolute_error
# - std_absolute_error
# - mean_relative_error
# - std_relative_error
metrics:
  - mean_absolute_error
  - std_absolute_error
  - mean_relative_error
  - std_relative_error