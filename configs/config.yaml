# =============================================================================
# PREPARE DATA
# =============================================================================

prepare_data:
  input_output_data:
    file_path: data/input_output_data.csv
    
    create_cols: 
      rho_start: '1 - 1/df.keff_start'
      rho_max: '1 - 1/df.keff_max'

      series:
        rhoN: '1 - 1/df.keffN'

  # Possible values: 1/4 and 1/8
  symmetry: 1/4
  one_hot_encoding: &one_hot_encoding True

  # Possible outputs:
  # - keff_start
  # - keff_max
  # - ppf_start
  # - ppf_max
  # - ppf_end
  # - cycle_length_in_days
  # - rho_start
  # - rho_max
  # - keff_history
  # - rho_history
  output_cols: &output_cols
    - rho_max


# =============================================================================
# USING MONOCORES DICTIONARY
# =============================================================================

use_monocores:
  execute: True
  one_hot_encoding: *one_hot_encoding

  monocre_data:
    file_path: data/monocores_dictionary.xlsx
    sheet_name: summary
    create_cols: 
      rho_start: '1 - 1/df.keff_start'
      rho_max: '1 - 1/df.keff_max'

  core_number_column: monocore

  # Possible transform colum names:
  # - cycle_length_in_days
  # - keff_max
  # - Pxy_max
  # - Pz_max
  # - keff_start
  # - PPF_start
  # - PPF_max
  # - PPF_end
  # - rho_start
  # - rho_max
  transform_col_names:
    - cycle_length_in_days
    - Pxy_max
    - Pz_max
    - PPF_max
    - PPF_end
    - rho_start
    - rho_max


# =============================================================================
# MODELING
# =============================================================================

# Possible activation functions:
# - relu
# - sigmoid
# - softmax
# - softplus
# - softsign
# - tanh
# - selu
# - elu
# - exponential
# - linear

modeling:
  data:
    train_split: 0.2

  neural_network_layout:
    normalize: True

    layers:
      layer1:
        neurons: 50
        activation: linear

      layer2:
        neurons: 50
        activation: linear

      layer_output:
        activation: linear

  neural_network_compile:
    loss_function: mean_squared_error
    learning_rate: 0.000001

  neural_network_learning:
    epochs: 100


# =============================================================================
# EVALUATION OF THE MODEL
# =============================================================================

evaluate:
  output_cols: *output_cols

  # Possible metrics:
  # - mean_absolute_error
  # - std_absolute_error
  # - mean_relative_error
  # - std_relative_error
  metrics:
    - mean_absolute_error
    - std_absolute_error
    - mean_relative_error
    - std_relative_error
  